# -*- coding: utf-8 -*-
"""training-finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sK3dMY9uhTIhyIWaPIqAdqHC0sK2pLoR
"""

import os
import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split

# Specify the full path to the directory containing language folders on your drive
path = '/kaggle/input/13datasets/Data'  # Change this to the actual path on your drive

# Define the folder names (in lowercase) and corresponding labels for programming languages
data_dirs = ['cpp', 'json', 'java', 'javascript', 'groovy', 'python','xml','yml','sql','scala','go','php','swift']
labels = ['cpp', 'json', 'java', 'javascript', 'groovy', 'python','xml','yml','sql','scala','go','php','swift']

# Initialize the tokenizer and model for CodeBERT
tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')
model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=len(labels))

# Function to load and filter files by UTF-8 encoding
def load_data_and_filter(path, data_dirs, labels):
    texts = []
    targets = []

    # Initialize a dictionary to count files per folder
    file_counts = {folder: 0 for folder in data_dirs}

    for idx, folder in enumerate(data_dirs):
        folder_path = os.path.join(path, folder)

        # Check if the folder exists
        if not os.path.exists(folder_path):
            print(f"Folder {folder} does not exist at {folder_path}")
            continue

        for filename in os.listdir(folder_path):
            abs_file = os.path.join(folder_path, filename)

            try:
                # Attempt to open the file with UTF-8 encoding
                with open(abs_file, encoding='utf-8', mode='r') as f:
                    texts.append(f.read())  # If the file is read without error, it's in UTF-8
                    targets.append(idx)      # Append the corresponding label
                    file_counts[folder] += 1 # Increment the count for the folder
            except UnicodeDecodeError:
                # If a UnicodeDecodeError occurs, print the filename and remove the file
                print(f"Removing non-UTF-8 file: {abs_file}")
                os.remove(abs_file)  # Remove non-UTF-8 file

    return texts, targets, file_counts

# Load and filter the dataset
texts, targets, file_counts = load_data_and_filter(path, data_dirs, labels)

# Print the number of files left in each folder after filtering
print("Number of files left in each folder after filtering non-UTF-8 files:")
for folder, count in file_counts.items():
    print(f"{folder}: {count} files")

# Split the data into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(texts, targets, test_size=0.2, random_state=42)

# Tokenize the texts using the CodeBERT tokenizer
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)

# Convert the tokenized data into Dataset objects
train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': train_labels})
val_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask'], 'labels': val_labels})

# Create a DatasetDict
dataset = DatasetDict({
    'train': train_dataset,
    'validation': val_dataset
})

# # Define training arguments for fine-tuning CodeBERT
# training_args = TrainingArguments(
#     output_dir='./results',
#     num_train_epochs=5,
#     per_device_train_batch_size=8,
#     per_device_eval_batch_size=8,
#     warmup_steps=500,
#     weight_decay=0.01,
#     logging_dir='./logs',
#     logging_steps=10,
#     evaluation_strategy="epoch"
# )

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=5,  # Increase number of epochs
    per_device_train_batch_size=16,  # Larger batch size
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="steps",  # Evaluate more frequently
    eval_steps=500,
    learning_rate=2e-5,  # Lower learning rate
    gradient_accumulation_steps=4,  # Simulate larger batches
    fp16=True,  # Mixed precision training
    load_best_model_at_end=True,  # Load the best model based on validation
    save_total_limit=2,  # Keep only the last two models to save disk space
)


# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer
)

"""# ** 4b4a3e9b42499d54789ab954b1bab949873b3d2f api key**"""

# Fine-tune the model
trainer.train()

# Save the fine-tuned model and tokenizer
model.save_pretrained("/kaggle/working/fine_tuned_codebertj_")
tokenizer.save_pretrained("/kaggle/working/fine_tuned_codebertj_")

# Evaluate the fine-tuned model on the validation set
trainer.evaluate()

import shutil

# Path to the directory containing the fine-tuned model and tokenizer
model_dir = "/kaggle/working/fine_tuned_codebertj_"

# Path where the zip file will be saved
zip_file_path = "/kaggle/working/fine_tuned_codebertj_.zip"

# Zip the folder
shutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', model_dir)

print(f"Model and tokenizer zipped to: {zip_file_path}")

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

# Evaluate the model on the validation set
eval_results = trainer.predict(val_dataset)

# Extract predicted labels and true labels
predicted_labels = np.argmax(eval_results.predictions, axis=1)
true_labels = eval_results.label_ids

# Generate confusion matrix and classification report
conf_matrix = confusion_matrix(true_labels, predicted_labels)
class_report = classification_report(true_labels, predicted_labels, target_names=labels)

# Save confusion matrix and classification report to a text file
output_file = '/kaggle/working/results.txt'
with open(output_file, 'w') as f:
    f.write("Predicted on {} files. Results are as follows:\n\n".format(len(true_labels)))

    f.write("Confusion Matrix:\n")
    np.savetxt(f, conf_matrix, fmt='%d', delimiter='\t')  # Save confusion matrix with tab-separated values

    f.write("\nClassification Report:\n")
    f.write(class_report)

# Print confirmation
print(f"Results saved to {output_file}")

