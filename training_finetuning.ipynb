{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"2ba7204b-5d08-4d06-8a51-6dda7f967fea","_uuid":"5b6a3cd3-5ab2-4efc-8199-b97b64fc1ad6","collapsed":false,"execution":{"iopub.execute_input":"2024-10-12T05:36:21.924776Z","iopub.status.busy":"2024-10-12T05:36:21.924466Z","iopub.status.idle":"2024-10-12T05:36:45.790657Z","shell.execute_reply":"2024-10-12T05:36:45.789747Z","shell.execute_reply.started":"2024-10-12T05:36:21.924742Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c76cca738b7464f9a1ce13cb5a3eb19","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0a16e86a5b944ff8958f2ea842b92e8","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"844b701d88a440e98519d6c0d98385ba","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dea177cb3a6d40bcab3e1341aaf608c9","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c5d9b906b7e4ad681cc8c906ca74921","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc5060243c7d41929713cfbfff521460","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import os\n","import torch\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n","from datasets import Dataset, DatasetDict\n","from sklearn.model_selection import train_test_split\n","\n","# Specify the full path to the directory containing language folders on your drive\n","path = '/kaggle/input/13datasets/Data'  # Change this to the actual path on your drive\n","\n","# Define the folder names (in lowercase) and corresponding labels for programming languages\n","data_dirs = ['cpp', 'json', 'java', 'javascript', 'groovy', 'python','xml','yml','sql','scala','go','php','swift']\n","labels = ['cpp', 'json', 'java', 'javascript', 'groovy', 'python','xml','yml','sql','scala','go','php','swift']\n","\n","# Initialize the tokenizer and model for CodeBERT\n","tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n","model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=len(labels))\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T05:36:45.793300Z","iopub.status.busy":"2024-10-12T05:36:45.792515Z","iopub.status.idle":"2024-10-12T05:36:45.801629Z","shell.execute_reply":"2024-10-12T05:36:45.800599Z","shell.execute_reply.started":"2024-10-12T05:36:45.793252Z"},"trusted":true},"outputs":[],"source":["\n","# Function to load and filter files by UTF-8 encoding\n","def load_data_and_filter(path, data_dirs, labels):\n","    texts = []\n","    targets = []\n","    \n","    # Initialize a dictionary to count files per folder\n","    file_counts = {folder: 0 for folder in data_dirs}\n","\n","    for idx, folder in enumerate(data_dirs):\n","        folder_path = os.path.join(path, folder)\n","        \n","        # Check if the folder exists\n","        if not os.path.exists(folder_path):\n","            print(f\"Folder {folder} does not exist at {folder_path}\")\n","            continue\n","        \n","        for filename in os.listdir(folder_path):\n","            abs_file = os.path.join(folder_path, filename)\n","            \n","            try:\n","                # Attempt to open the file with UTF-8 encoding\n","                with open(abs_file, encoding='utf-8', mode='r') as f:\n","                    texts.append(f.read())  # If the file is read without error, it's in UTF-8\n","                    targets.append(idx)      # Append the corresponding label\n","                    file_counts[folder] += 1 # Increment the count for the folder\n","            except UnicodeDecodeError:\n","                # If a UnicodeDecodeError occurs, print the filename and remove the file\n","                print(f\"Removing non-UTF-8 file: {abs_file}\")\n","                os.remove(abs_file)  # Remove non-UTF-8 file\n","    \n","    return texts, targets, file_counts\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T05:36:45.803402Z","iopub.status.busy":"2024-10-12T05:36:45.803032Z","iopub.status.idle":"2024-10-12T05:37:40.622159Z","shell.execute_reply":"2024-10-12T05:37:40.621195Z","shell.execute_reply.started":"2024-10-12T05:36:45.803350Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of files left in each folder after filtering non-UTF-8 files:\n","cpp: 309 files\n","json: 6368 files\n","java: 592 files\n","javascript: 595 files\n","groovy: 86 files\n","python: 473 files\n","xml: 1985 files\n","yml: 6903 files\n","sql: 35 files\n","scala: 78 files\n","go: 97 files\n","php: 71 files\n","swift: 82 files\n"]}],"source":["\n","# Load and filter the dataset\n","texts, targets, file_counts = load_data_and_filter(path, data_dirs, labels)\n","\n","# Print the number of files left in each folder after filtering\n","print(\"Number of files left in each folder after filtering non-UTF-8 files:\")\n","for folder, count in file_counts.items():\n","    print(f\"{folder}: {count} files\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T05:37:40.625392Z","iopub.status.busy":"2024-10-12T05:37:40.624952Z","iopub.status.idle":"2024-10-12T05:54:10.992826Z","shell.execute_reply":"2024-10-12T05:54:10.991897Z","shell.execute_reply.started":"2024-10-12T05:37:40.625357Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"]}],"source":["\n","# Split the data into training and validation sets\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, targets, test_size=0.2, random_state=42)\n","\n","# Tokenize the texts using the CodeBERT tokenizer\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n","\n","# Convert the tokenized data into Dataset objects\n","train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': train_labels})\n","val_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask'], 'labels': val_labels})\n","\n","# Create a DatasetDict\n","dataset = DatasetDict({\n","    'train': train_dataset,\n","    'validation': val_dataset\n","})\n","\n","# # Define training arguments for fine-tuning CodeBERT\n","# training_args = TrainingArguments(\n","#     output_dir='./results',\n","#     num_train_epochs=5,\n","#     per_device_train_batch_size=8,\n","#     per_device_eval_batch_size=8,\n","#     warmup_steps=500,\n","#     weight_decay=0.01,\n","#     logging_dir='./logs',\n","#     logging_steps=10,\n","#     evaluation_strategy=\"epoch\"\n","# )\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=5,  # Increase number of epochs\n","    per_device_train_batch_size=16,  # Larger batch size\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    evaluation_strategy=\"steps\",  # Evaluate more frequently\n","    eval_steps=500,\n","    learning_rate=2e-5,  # Lower learning rate\n","    gradient_accumulation_steps=4,  # Simulate larger batches\n","    fp16=True,  # Mixed precision training\n","    load_best_model_at_end=True,  # Load the best model based on validation\n","    save_total_limit=2,  # Keep only the last two models to save disk space\n",")\n","\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer\n",")\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T05:54:10.994333Z","iopub.status.busy":"2024-10-12T05:54:10.994059Z","iopub.status.idle":"2024-10-12T06:56:30.883102Z","shell.execute_reply":"2024-10-12T06:56:30.882206Z","shell.execute_reply.started":"2024-10-12T05:54:10.994302Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ed7154669fc450097173b204a0452c3","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112723577778323, max=1.0â€¦"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241012_055432-tp5jb7vm</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/sabbinenisivagopikrishna-ibm/huggingface/runs/tp5jb7vm' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/sabbinenisivagopikrishna-ibm/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/sabbinenisivagopikrishna-ibm/huggingface' target=\"_blank\">https://wandb.ai/sabbinenisivagopikrishna-ibm/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/sabbinenisivagopikrishna-ibm/huggingface/runs/tp5jb7vm' target=\"_blank\">https://wandb.ai/sabbinenisivagopikrishna-ibm/huggingface/runs/tp5jb7vm</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='550' max='550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [550/550 1:01:47, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.017400</td>\n","      <td>0.047175</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=550, training_loss=0.5306421267715368, metrics={'train_runtime': 3738.3265, 'train_samples_per_second': 18.911, 'train_steps_per_second': 0.147, 'total_flos': 1.851958497429504e+16, 'train_loss': 0.5306421267715368, 'epoch': 4.97737556561086})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Fine-tune the model\n","trainer.train()\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T06:56:30.884630Z","iopub.status.busy":"2024-10-12T06:56:30.884319Z","iopub.status.idle":"2024-10-12T06:56:31.660054Z","shell.execute_reply":"2024-10-12T06:56:31.659144Z","shell.execute_reply.started":"2024-10-12T06:56:30.884597Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('/kaggle/working/fine_tuned_codebertj_/tokenizer_config.json',\n"," '/kaggle/working/fine_tuned_codebertj_/special_tokens_map.json',\n"," '/kaggle/working/fine_tuned_codebertj_/vocab.json',\n"," '/kaggle/working/fine_tuned_codebertj_/merges.txt',\n"," '/kaggle/working/fine_tuned_codebertj_/added_tokens.json')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Save the fine-tuned model and tokenizer\n","model.save_pretrained(\"/kaggle/working/fine_tuned_codebertj_\")\n","tokenizer.save_pretrained(\"/kaggle/working/fine_tuned_codebertj_\")\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T06:56:31.661373Z","iopub.status.busy":"2024-10-12T06:56:31.661070Z","iopub.status.idle":"2024-10-12T06:57:32.709366Z","shell.execute_reply":"2024-10-12T06:57:32.708477Z","shell.execute_reply.started":"2024-10-12T06:56:31.661340Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.047174595296382904,\n"," 'eval_runtime': 61.0366,\n"," 'eval_samples_per_second': 57.916,\n"," 'eval_steps_per_second': 1.819,\n"," 'epoch': 4.97737556561086}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Evaluate the fine-tuned model on the validation set\n","trainer.evaluate()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T06:57:32.710884Z","iopub.status.busy":"2024-10-12T06:57:32.710519Z","iopub.status.idle":"2024-10-12T06:58:00.820332Z","shell.execute_reply":"2024-10-12T06:58:00.819407Z","shell.execute_reply.started":"2024-10-12T06:57:32.710849Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model and tokenizer zipped to: /kaggle/working/fine_tuned_codebertj_.zip\n"]}],"source":["import shutil\n","\n","# Path to the directory containing the fine-tuned model and tokenizer\n","model_dir = \"/kaggle/working/fine_tuned_codebertj_\"\n","\n","# Path where the zip file will be saved\n","zip_file_path = \"/kaggle/working/fine_tuned_codebertj_.zip\"\n","\n","# Zip the folder\n","shutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', model_dir)\n","\n","print(f\"Model and tokenizer zipped to: {zip_file_path}\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T06:58:00.823739Z","iopub.status.busy":"2024-10-12T06:58:00.823430Z","iopub.status.idle":"2024-10-12T06:59:02.469293Z","shell.execute_reply":"2024-10-12T06:59:02.468445Z","shell.execute_reply.started":"2024-10-12T06:58:00.823705Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Results saved to /kaggle/working/results.txt\n"]}],"source":["import numpy as np\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","# Evaluate the model on the validation set\n","eval_results = trainer.predict(val_dataset)\n","\n","# Extract predicted labels and true labels\n","predicted_labels = np.argmax(eval_results.predictions, axis=1)\n","true_labels = eval_results.label_ids\n","\n","# Generate confusion matrix and classification report\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","class_report = classification_report(true_labels, predicted_labels, target_names=labels)\n","\n","# Save confusion matrix and classification report to a text file\n","output_file = '/kaggle/working/results.txt'\n","with open(output_file, 'w') as f:\n","    f.write(\"Predicted on {} files. Results are as follows:\\n\\n\".format(len(true_labels)))\n","    \n","    f.write(\"Confusion Matrix:\\n\")\n","    np.savetxt(f, conf_matrix, fmt='%d', delimiter='\\t')  # Save confusion matrix with tab-separated values\n","    \n","    f.write(\"\\nClassification Report:\\n\")\n","    f.write(class_report)\n","\n","# Print confirmation\n","print(f\"Results saved to {output_file}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5844690,"sourceId":9584498,"sourceType":"datasetVersion"},{"datasetId":5860864,"sourceId":9605974,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
